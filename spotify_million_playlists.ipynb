{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StTEshrVOhTk"
      },
      "source": [
        "### Ferdinand Hoske\n",
        "# Spotify Million Playlist Challenge\n",
        "\n",
        "For tis project  I decided to work on the Million Playlist Challenge held by Spotify in 2018. Spotify made the dataset public in September 2020. The goal of the challenge is to recommend songs that suit a specific playlist. The so called automatic playlist continuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yabUH0Ri6yQR",
        "outputId": "651b4241-f5a1-42ef-f872-203ad58e80ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tekore in /usr/local/lib/python3.7/dist-packages (3.6.2)\n",
            "Requirement already satisfied: lightfm in /usr/local/lib/python3.7/dist-packages (1.16)\n",
            "Requirement already satisfied: httpx<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from tekore) (0.17.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightfm) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from lightfm) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.19.5)\n",
            "Requirement already satisfied: httpcore<0.13,>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from httpx<0.18,>=0.11->tekore) (0.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx<0.18,>=0.11->tekore) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx<0.18,>=0.11->tekore) (2020.12.5)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx<0.18,>=0.11->tekore) (1.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2.10)\n",
            "Requirement already satisfied: h11==0.* in /usr/local/lib/python3.7/dist-packages (from httpcore<0.13,>=0.12.1->httpx<0.18,>=0.11->tekore) (0.12.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tekore lightfm\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse as sp\n",
        "import json\n",
        "import glob\n",
        "import tekore as tk\n",
        "import progressbar as pb\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-YqL6NLP3OG"
      },
      "source": [
        "## 1. Datapreprocessing and Feature Engineering\n",
        "The original dataset holds 1.000.000 playlists and over 2 Million unique tracks. For performance reasons we will work with a portion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWT70Mky65NJ",
        "outputId": "685206a1-3231-4184-bdfb-02fa1cfe73fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "files = glob.glob(\"/content/drive/My Drive/spotify playlist challenge/unzip/data/frac/*\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLbHGRT7RBQU"
      },
      "source": [
        "We first load the JSON files holding the playlists and concat them in a DataFrame called `data_playlists`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ITrW05PP66-S"
      },
      "outputs": [],
      "source": [
        "data_playlists = pd.DataFrame()\n",
        "for file in files:\n",
        "  single_json = json.load(open(file))\n",
        "  df = pd.DataFrame(single_json[\"playlists\"])\n",
        "  data_playlists = data_playlists.append(df, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6wNeFuXRSbY"
      },
      "source": [
        "The Songs are saved as dict-like object inside the \"tracks\" column of each playlist. We want to have two additional DataFrames. One holding the interaction between playlist and track called `tracks_playlist`\n",
        " and one that holds each unique track with additional information called `data_tracks`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBJf7Ixl6-K1",
        "outputId": "4022abbd-2a52-430b-b942-eb6ca1956d3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "tracks_playlist = pd.DataFrame()\n",
        "data_tracks = pd.DataFrame()\n",
        "\n",
        "#iterates through playlist and saves songs in DataFrame\n",
        "for i, playlist in data_playlists.iterrows():\n",
        "  tracks_df = pd.DataFrame(playlist[\"tracks\"])\n",
        "  tracks_df[\"playlist_id\"] = playlist[\"pid\"]\n",
        "  tracks_playlist = tracks_playlist.append(tracks_df, ignore_index=True)\n",
        "\n",
        "#creates a new DataFrame that holds information of unique songs\n",
        "data_tracks = tracks_playlist[tracks_playlist[\"track_uri\"].duplicated() != True]\n",
        "\n",
        "#change uri to id and drop columns that are not neccessary for DataFrame\n",
        "data_tracks[\"id\"] = data_tracks[\"track_uri\"].str.lstrip(\"spotify:track:\").values\n",
        "tracks_playlist[\"track_id\"] = tracks_playlist[\"track_uri\"].str.lstrip(\"spotify:track:\").values\n",
        "data_playlists = data_playlists.drop(\"tracks\", 1)\n",
        "data_tracks = data_tracks.drop([\"playlist_id\", \"pos\", \"track_uri\"], 1)\n",
        "tracks_playlist = tracks_playlist.drop(\n",
        "    [\"artist_name\", \"artist_uri\", \"track_name\", \"album_uri\", \"duration_ms\", \"album_name\", \"track_uri\"], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSMQGyCSW8o"
      },
      "source": [
        "Additional to the data provided by the dataset we want to use the Spotify API to enrich the track information with **Audio Features**. The package tekore provides the methods to access the HTTP routes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cWa7Gr3XwaNm"
      },
      "outputs": [],
      "source": [
        "features = [\"danceability\", \n",
        "            \"energy\", \n",
        "            \"speechiness\", \n",
        "            \"acousticness\", \n",
        "            \"instrumentalness\", \n",
        "            \"liveness\", \n",
        "            \"tempo\", \n",
        "            \"valence\"]\n",
        "\n",
        "app_token = tk.request_client_token(\"76e5eea6048a40b3b12da575781777a0\", \"4e294eb734dc462c992028985b40c930\")\n",
        "spotify = tk.Spotify(token=app_token, sender=tk.RetryingSender(retries=3))\n",
        "\n",
        "tracks_features = pd.DataFrame()\n",
        "\n",
        "#Access API with 100 track ids which is the maximum amount per call\n",
        "i = 0\n",
        "while i<len(data_tracks[\"id\"].values):\n",
        "  j = i + 100\n",
        "  sp_features = spotify.tracks_audio_features(data_tracks[\"id\"].values[i:j])\n",
        "  tracks_features = tracks_features.append(sp_features)\n",
        "  i = j\n",
        "\n",
        "data_tracks[features] = tracks_features[features].values\n",
        "\n",
        "#normalize tempo so it fits the other features scale from 0 to 1\n",
        "data_tracks[\"tempo\"] = (data_tracks[\"tempo\"].values - data_tracks[\"tempo\"].min())/(data_tracks[\"tempo\"].max() - data_tracks[\"tempo\"].min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy8TUFJayRd0"
      },
      "source": [
        "## 2. Model Evaluation Method\n",
        "The challenge was evaluated based on different metrics  defined by Spotify which we also will use to evaluate or model.\n",
        "### R-Precision\n",
        "R-Precision is the number of predicted songs that match the ground truth devided by all songs in the playlist and averaged over all playlists.\n",
        "\n",
        "$\\text{R-precision}=\\frac{|G\\cap R|}{|R|}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mNs2iq7a1wzW"
      },
      "outputs": [],
      "source": [
        "def get_r_precision(recommendations, playlist_track_list):\n",
        "  playlist_length = playlist_track_list.size\n",
        "  match = np.count_nonzero(np.in1d(recommendations, playlist_track_list))\n",
        "  return match/playlist_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwGYoXfN2gCG"
      },
      "source": [
        "We won't score the Normalized Discounted Cumulative Gain because we are missing the weights to calculate this metric. We also won't consider artist match becasue we are missing how they are weighted either."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RanYI2zNS8Ws"
      },
      "source": [
        "## 3. Simple Recommender Model\n",
        "The first approach is going to be a purely collaborative filtering method where we check for correlation between a song and each song in the playlist we want to recommend for. To calculate the correlation we use the Phi-Coefficient.\n",
        "\n",
        "$\\phi=\\frac{n_{11}n_{00}-n_{10}n_{01}}{\\sqrt{n_{\\bullet 1}n_{1\\bullet}n_{\\bullet 0}n_{0\\bullet}}}$\n",
        "\n",
        "$n_{11}$ is the number of times the songs appear in the same playlist, $n_{00}$ how often they both dont appear in a playlist, $n_{10}$ the number of times the first song appears in a playlist the other isnt and $n_{01}$ vice versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eY5MWhCmUm41"
      },
      "outputs": [],
      "source": [
        "def phi(a, b):\n",
        "  a = M.loc[a].values\n",
        "  b = M.loc[b].values\n",
        "  n11 = len(np.where((a==1) & (b==1))[0])\n",
        "  n10 = len(np.where((a==1) & (b==0))[0])\n",
        "  n01 = len(np.where((a==0) & (b==1))[0])\n",
        "  n00 = len(np.where((a==0) & (b==0))[0])\n",
        "  return (n11*n00-n10*n01)/np.sqrt((n11+n01)*(n01+n00)*(n11+n10)*(n10+n00))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53YoZ8X2UsE8"
      },
      "source": [
        "To feed the function we need a pivot table where the tracks are the rows and the playlists the columns and a 1 representing an appearance and a 0 an absence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-borYb0HVghg"
      },
      "outputs": [],
      "source": [
        "M = tracks_playlist.pivot_table(columns=[\"playlist_id\"], index=[\"track_id\"])\n",
        "M.iloc[~M.isna()] = 1\n",
        "M = M.fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvrIXANWVzUO"
      },
      "source": [
        "To get recommendations we calculate the phi coefficient of each track in our `data_tracks` DataFrame that is not included in our playlist and return the `track_id` in the order of the highest score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Jw7TVPINV3ou"
      },
      "outputs": [],
      "source": [
        "def get_phi_recommendation(playlist_id):\n",
        "  playlist_tracks = tracks_playlist[\"track_id\"].values[tracks_playlist[\"playlist_id\"] == playlist_id]\n",
        "\n",
        "  func = lambda track: [track, np.sum([phi(x, track) for x in playlist_tracks if track not in playlist_tracks])/len(playlist_tracks)]\n",
        "  recommendations = list(map(func, tracks_playlist[\"track_id\"].unique()))\n",
        "\n",
        "  recommendations.sort(key=lambda k: k[1], reverse= True)\n",
        "  return recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxEXL71HW_Ti"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "recommendations = get_phi_recommendation(45)\n",
        "print(time.time()-start)\n",
        "recommendations[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WeHvH0FXH4n"
      },
      "source": [
        "By self-testing the recommended songs the recommender gave good results but had two major downsides:\n",
        "1. It was a lot of computing necessary.\n",
        "2. You couldn't really score the results others than subjectively rate the recommendations because you obvsiously the songs of the playlist would always have the highest correlation so there is no ground truth for what to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI-xUfwyYniD"
      },
      "source": [
        "## 4. Simple Matrix Factorization Model\n",
        " The second approach is the Matrix Factorization. Similar to the concept of Single Value Decomposition are we aiming to factorize two matrices P and Q that hold latent features of our playlists and tracks.\n",
        "\n",
        "We use stoachstic gradient descent to minimize the L2 loss for every element of P and Q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B2JylSHialFd"
      },
      "outputs": [],
      "source": [
        "def matrix_factorization(M, K=5, steps=100, alpha=0.02, beta=0.02): \n",
        "  P = np.random.rand(M.shape[0],K)\n",
        "  Q = np.random.rand(M.shape[1],K).T\n",
        "  for step in pb.progressbar(range(steps)):\n",
        "    for i in range(len(M)):\n",
        "      for j in range(len(M[i])):\n",
        "        if M[i][j] > 0:\n",
        "          eij = M[i][j] - np.dot(P[i,:],Q[:,j])\n",
        "          for k in range(K):\n",
        "            P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
        "            Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
        "    l = 0\n",
        "    for i in range(len(M)):\n",
        "      for j in range(len(M[i])):\n",
        "        if M[i][j] > 0:\n",
        "          l = l + pow(M[i][j] - np.dot(P[i,:],Q[:,j]), 2)\n",
        "          for k in range(K):\n",
        "            l = l + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))\n",
        "    if l < 0.001:\n",
        "      break\n",
        "  return P, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTY5oniebQZJ"
      },
      "source": [
        "The dot product of the row or column of P and Q then represent how good a song would suit the playlist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4_Abfq3cOU3"
      },
      "outputs": [],
      "source": [
        "P_hat, Q_hat = matrix_factorization(M.values)\n",
        "\n",
        "pid = 45\n",
        "playlist_track_list = tracks_playlist[\"track_id\"].values[tracks_playlist[\"playlist_id\"] == pid]\n",
        "recommendations = M.index[np.dot(P_hat, Q_hat[:,pid]).argsort()[::-1]][:500]\n",
        "print(\"R-Precision for simple Matrix Multiplication: \", get_r_precision(recommendations, playlist_track_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6YmRrb8dMKk"
      },
      "source": [
        "The results ware rather unsatisfying which could be due to the sparseness of the matrix, the binary of the data or the choice of loss function. The computation also took a lot of time. We further couldn't use this model for unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGnQFl89dnnc"
      },
      "source": [
        "## 5. Hybrid Recommender Model\n",
        "For our final approach we will use the popular package lightFM to build a Matrix Factorization model but enrich it with the audio features we gathered from the Spotify API to therefore build a hybrid collaborative and content based filtering model. \n",
        "\n",
        "The predict method of lightFM allows us to also work with unseen data. We therefore split our tracks in training and test data. The interaction table to train the data is basically the same as the pivot table but we use the `build_interactions` method for easier indexing capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WLyA5RFC4RHo"
      },
      "outputs": [],
      "source": [
        "#set train test split\n",
        "train_split = 0.75\n",
        "number_train_playlists = int(len(data_playlists)*train_split)\n",
        "train_ids = np.arange(number_train_playlists)\n",
        "test_ids = np.arange(number_train_playlists, len(data_playlists))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "StyRPGDJrsqg"
      },
      "outputs": [],
      "source": [
        "dataset= Dataset()\n",
        "dataset.fit(\n",
        "    users = train_ids, \n",
        "    items = tracks_playlist[\"track_id\"].unique(),\n",
        "    item_features = features\n",
        ")\n",
        "train, weights = dataset.build_interactions([(x[1], x[2]) for x in tracks_playlist.values[tracks_playlist[\"playlist_id\"] < number_train_playlists]])\n",
        "\n",
        "# Save track_features in lightfm specific format [(item_id, {feature1: value1, ...}), ...]\n",
        "track_tuple= list(zip(data_tracks[\"id\"], data_tracks[features].to_dict('records')))\n",
        "item_features = dataset.build_item_features(track_tuple, normalize=False)\n",
        "\n",
        "user_id_map, user_feature_map, item_id_map, item_feature_map = dataset.mapping()\n",
        "item_id_map_rev = {value:key for key, value in item_id_map.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6H_nsFdhOc_"
      },
      "source": [
        "For the challenge Spotify provided a challenge dataset with incomplete playlists to validate the model. These playlists follow the following structure\n",
        "* first track\n",
        "* first 5 tracks\n",
        "* first 10 tracks\n",
        "* first 25 tracks\n",
        "* first 100 tracks\n",
        "\n",
        "plus the combination with a missing title. For this model we are focusing on predicitng based on tracks. The complete dataset contains over 2 million songs. To have an overlapping of songs and the ground truth information we are not using the challenge dataset but build interactions based on the given structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wumcQ8dbcXII"
      },
      "outputs": [],
      "source": [
        "test_set = np.empty(3)\n",
        "for id in test_ids:\n",
        "  playlist_track_list = tracks_playlist.values[tracks_playlist[\"playlist_id\"] == id]\n",
        "\n",
        "  if len(playlist_track_list)>=200:\n",
        "    test_set = np.vstack([test_set, playlist_track_list[:100]])\n",
        "  elif len(playlist_track_list)>=100:\n",
        "    test_set = np.vstack([test_set, playlist_track_list[:25]])\n",
        "  elif len(playlist_track_list)>=50:\n",
        "    test_set = np.vstack([test_set, playlist_track_list[:10]])\n",
        "  elif len(playlist_track_list)>=25:\n",
        "    test_set = np.vstack([test_set, playlist_track_list[:5]])\n",
        "  elif len(playlist_track_list)<25:\n",
        "    test_set = np.vstack([test_set, playlist_track_list[:1]])\n",
        "\n",
        "test_set = np.delete(test_set, (0), axis=0)\n",
        "\n",
        "test_dataset = Dataset()\n",
        "test_dataset.fit(\n",
        "    users = test_ids, \n",
        "    items = tracks_playlist[\"track_id\"].unique(),\n",
        "    item_features = features\n",
        ")\n",
        "test, weights = test_dataset.build_interactions(\n",
        "    [(x[1], x[2]) for x in test_set])\n",
        "\n",
        "test_user_id_map, test_user_feature_map, test_item_id_map, test_item_feature_map = test_dataset.mapping()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDcyGcg7wETn"
      },
      "source": [
        "To train our model we use 200 components to describe playlists and songs, a slightly slower learning rate, an initial state of one for creating the random parameters and the **Weighted Approximate-Rank Pairwise** loss. This has been shown to be useful for recommendation models and was also used by some of the participants in the 2018 challenge. To speed up training we set the maximum number of negative samples to 200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byIgLBY9C4bP",
        "outputId": "c0201919-4eb5-43fe-846c-565f089c3602"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<lightfm.lightfm.LightFM at 0x7fbf07f71d10>"
            ]
          },
          "execution_count": 18,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LightFM(\n",
        "    no_components=200, \n",
        "    loss='warp', \n",
        "    learning_rate=0.02, \n",
        "    max_sampled=400, \n",
        "    random_state=1, \n",
        "    user_alpha=1e-05)\n",
        "\n",
        "model.fit(\n",
        "    interactions=train, \n",
        "    item_features=item_features, \n",
        "    epochs=20, \n",
        "    num_threads=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5Mttmstn7D5l"
      },
      "outputs": [],
      "source": [
        "def get_lightfm_recommendation(item):\n",
        "  predicitions = model.predict(\n",
        "      user_ids=0, \n",
        "      item_ids=np.arange(data_tracks.shape[0]), \n",
        "      item_features=item, \n",
        "      num_threads=2\n",
        "  ).argsort()[::-1]\n",
        "  return [item_id_map_rev[predicitions[i]] for i in range(500)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r4reTgzlBZ2",
        "outputId": "031e58c8-c55a-4b45-de7a-bac82eb316db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R-Precision for Traindata:  0.990830198148823\n"
          ]
        }
      ],
      "source": [
        "r_values = []\n",
        "\n",
        "for pid in train_ids:\n",
        "  fm_id = user_id_map[pid]\n",
        "  item = train.getrow(fm_id).transpose()\n",
        "  playlist_track_list = tracks_playlist[\"track_id\"].values[tracks_playlist[\"playlist_id\"] == pid]\n",
        "\n",
        "  recommendations = np.array(get_lightfm_recommendation(item))\n",
        "  r_values.append(get_r_precision(recommendations, playlist_track_list))\n",
        "\n",
        "print(\"R-Precision for Traindata: \", np.sum(r_values)/len(train_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is93ewZB1B_k"
      },
      "source": [
        "The final step is to calculate the R-Precision for our test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkKYHKp0_JIv",
        "outputId": "2a40782f-7ad4-442b-f71c-6a3f6309bb62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R-Precision for Testdata:  0.16036571589559334\n"
          ]
        }
      ],
      "source": [
        "r_values = []\n",
        "\n",
        "for pid in test_ids:\n",
        "  fm_id = test_user_id_map[pid]\n",
        "  item = test.getrow(fm_id).transpose()\n",
        "  playlist_track_list = tracks_playlist[\"track_id\"].values[tracks_playlist[\"playlist_id\"] == pid]\n",
        "\n",
        "  recommendations = np.array(get_lightfm_recommendation(item))\n",
        "  r_values.append(get_r_precision(recommendations, playlist_track_list))\n",
        "\n",
        "print(\"R-Precision for Testdata: \", np.sum(r_values)/len(test_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri7-WhFM1fwa"
      },
      "source": [
        "## 6. Conclusion and Outlook\n",
        "During the challenge the top teams could reach a R-Precision between 0.22 and 0.2. If we compare them with our results of 0.16 our model did not excel but is not far of the leading research results either. It is to mention that during the challenge the score was also evaluated on playlists without tracks and only the title where the average was abut 0.09.\n",
        "\n",
        "Most participants also worked with a Two stage model where a neural network reranked the results of the matrix factorization. A second stage therefore could also enhance our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Take_two.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
